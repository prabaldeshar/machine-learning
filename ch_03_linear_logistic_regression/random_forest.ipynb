{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple decision tress via random forests\n",
    "\n",
    "A random forest can be considered as an **ensemble** of decision trees. The idea behind a random forest is to average multiple (deep) decision trees that individually suffer from high variance to build a more robust model that has a better generalization performance and is less susceptible to overfitting. The random forest algorithm can be summarized in four simple steps:\n",
    "1. Draw a random **bootstrap** sample of size *n* (randomly choose *n* examples from the training dataset with replacement).\n",
    "1. Grow a decision tree from the bootstrap sample. At each node: <br/>\n",
    "    - Randomly select *d* features without replacement. \n",
    "    - Split the node using the feature that provides the best split according to the objective fucntion, for instance, maximizing the information gain.\n",
    "\n",
    "1. Repeat the steps 1-2 $k$ times.\n",
    "1. Aggregate the prediction by each tree to assign the class label by majority vote.\n",
    "<br/>\n",
    "\n",
    "Decresing the size of the bootstrap sample increases the diversity among the inidividual trees, since the probablility that a particular training example is included in the bootstrap sample is lower.So, shriking the size of the bootstrap sampleas may increase the *randomness* of the random forest, and it can help reduce the effect of overfitting. However, smaller bootstrap samples typically result in a lower overall performance of the random forest, and a small gap between training and test performance, but low test performance overall. Conversly, increasing the size of the bootstrap sample may increase the degree of overfitting. Because the bootstrap samples, and consequently the individual decison trees, become more similar to each other, they learn to fit the original training dataset more closely.\n",
    "\n",
    "In most implementations, the size of the bootstrap sample is chosen to be equal to the number of training examples in the original dataset, which usually provides a good bias variance tradeoff.\n",
    "\n",
    "For the number of features, $d$, at each split, we want to choose a value that is smaller than the total number of features in the training dataset. A resonable default that in scikit-learn and other implementations is d = $ \\sqrt{m} $ where $m$ is the number of features in the training dataset.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
