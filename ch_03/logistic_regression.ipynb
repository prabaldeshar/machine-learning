{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function\n",
    "\n",
    "$\\hat y ^ {(i)} = f^{w, b} (x ^ {(i)})$ \n",
    "\n",
    "$f_{w, b} (x^{(i)}) = wx^{(i)} + b$ \n",
    "<br/>\n",
    "Squared error cost function\n",
    "\n",
    "$J(w, b) = \\frac{1}{2m} \\sum_{i=1} ^ {m} (\\hat y ^ {(i)} - y ^ {(i)}) ^ 2 $\n",
    "\n",
    "<br/>\n",
    "\n",
    "The above euqation can be written as , <br/>\n",
    "\n",
    "$J(w, b) = \\frac{1}{2m} \\sum_{i=1} ^ {m} (f_{w, b} (x^{(i)}) - y ^ {(i)}) ^ 2 $\n",
    "<br/>\n",
    "where, \n",
    "m = number of training examples\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Gradient descent for Liner Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Linear regression\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
